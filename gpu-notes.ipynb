{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a65a5e7-3620-4715-97e7-5a4bbcfb03c7",
   "metadata": {},
   "source": [
    "# Graphics Processing Units (GPUs)\n",
    "\n",
    "- Originally designed to perform computationally intensive graphics calculations\n",
    "- GPUs as we know them today are largely derived from the NVIDIA GeForce 256 card, which was the first consumer level GPU\n",
    "- Quickly, scientists and engineers recognized the similarities in many of their problems and those that the GPU was designed to be very good at. This led to what we now refer to as General Purpose GPU (GPGPU) programming.\n",
    "    - Any problem which requires the application of similar operations to a lot of data can benefit from the architecture of GPUs\n",
    "- Though GPU hardware is capable of accelerating computation in many fields other than just graphics, to take full advantage of GPUs a software platform that allows GPU programs to be written similarly to CPU programs. The first to solve this problem was NVIDIA with CUDA paradigm.\n",
    "    - Soon after a vendor-agnostic language was created (OpenCL)\n",
    "- CUDA and OpenCL turned GPUs into general purpose computing devices that can be leveraged alongside CPUs to perform complex calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa49a25-fa4f-4a57-97fb-e53e44c57938",
   "metadata": {},
   "source": [
    "# GPU vs. CPU\n",
    "\n",
    "- GPUs are \"external\" devices that must interface with a CPU \n",
    "- GPUs cores are usually around 2 times slower than CPUs (2-2.5 GHz compared to 4-6 GHz)\n",
    "- GPUs have **a lot** more cores than CPUs\n",
    "    - For example, an AMD Threadripper 3990X has 64 cores (which is about as much as you will find) while an NVIDIA RTX 3090 has more than 10000 CUDA cores. \n",
    "- This means that processes that are algorithmically single-threaded or not compute intensive will generally be faster on CPUs than GPUs and vice-versa.\n",
    "\n",
    "| ![CPU vs. GPU](./figures/gpu-devotes-more-transistors-to-data-processing.png) |\n",
    "|:--:|\n",
    "| Figure Credit - Figure 1 (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc3b47-c543-421e-b91b-3a21d1eb8456",
   "metadata": {},
   "source": [
    "# GPU Terminology\n",
    "\n",
    "- **Host** - the CPU that the GPU is connected to\n",
    "- **Device** - the GPU\n",
    "- **GPU Core** - processing unit of GPU\n",
    "- **GPU Thread** - execution context of a GPU core\n",
    "- **Kernel** - function created to be executed on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402f76c-ff78-48a3-aa8e-a3e7e29f8700",
   "metadata": {},
   "source": [
    "# Model for Parallelism on GPUs\n",
    "\n",
    "GPUs excel at what is commonly referred to as Single Instruction Multiple Data (SIMD) parallelism. This means that any time you write a loop to do computation, there is potential that it can be accelerated with a GPU. \n",
    "\n",
    "To best understand how GPUs are used, we can look at the CUDA programming paradigm. Though there are other ways to program GPUs (OpenACC, OpenCL), developing a basic understanding of CUDA makes using these other methods significantly easiser.\n",
    "\n",
    "- The basic idea is to parallelize iteration blocks by replacing the iteration variable with a thread index variable.\n",
    "    - So write the body of the loop as a separate function that is then executed on the GPU (**Kernel**)\n",
    "- Threads are organized into (possible multidimensional) **blocks**\n",
    "- Blocks are divided into **warps** where the number of threads in a warp is equal to the number of cores in a **streaming multiprocessor** (**SM**)\n",
    "    - SMs are just collections of cores, where each core has an arithemetic logic unit (ALU) and a floating point unit (FPU)\n",
    "- Warps are then executed on SMs\n",
    "- As programmers, we are usually only concerned with threads and blocks. For this reason, CUDA provides several predifined variables to help determine which thread is running what. \n",
    "    - `gridDim` - grid dimensions (in blocks, e.g., a grid can be 540 x 480 blocks)\n",
    "    - `blockDim` - block dimensions (in threads)\n",
    "    - `blockIdx` - block position within a grid\n",
    "    - `threadIdx` - thread position within a block\n",
    "    - e.g. To process a 4K image (2160 x 3840 pixels), you may choose a block size of 32 x 32 (choosing block sizes that are multiples of 32 is good for performance as that is the number of threads in an SM). With that block size the grid size will be 68 x 120 (ceil(2160/32), ceil(3840/32)). Note this is a 2 dimensional grid. So to get the x index: `int i_x = blockIdx.x * blockDim.x + threadIdx.x` and to get the y index: `int i_y = blockIdx.y * blockDim.y + threadIdx.y`.\n",
    "\n",
    "```C++\n",
    "#include <stdio.h>\n",
    "#define N 64\n",
    "#define TPB 32\n",
    "\n",
    "// any function the the __device__ qualifer is compiled to be\n",
    "// called from GPU and executed on GPU\n",
    "__device__ float scale(int i, int n) {\n",
    "    return ((float) i)/(n-1);\n",
    "}\n",
    "\n",
    "__device__ float distance(float x1, x2) {\n",
    "    return sqrt((x2 - x1) * (x2 - x1));\n",
    "}\n",
    "\n",
    "// functions with the __global__ qualifier is compiled to be\n",
    "// called from CPU and executed on GPU\n",
    "__global__ void distanceKernel(float *d_out, float ref, int len) {\n",
    "    const int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < len) {\n",
    "        const float x = scale(i, len);\n",
    "        d_out[i] = distance(x, ref);\n",
    "    }\n",
    "    // it is pretty simple to see how the const int i line\n",
    "    // is replacing a for (int i=0; i<N; ++i) loop\n",
    "    // this is often referred to as going from loops to grids\n",
    "}\n",
    "\n",
    "// you can also stack qualifiers so\n",
    "// __host__ __device__ functions will have a CPU\n",
    "// and CPU version compiled and the correct one will be ran \n",
    "// depending on where they are called on.\n",
    "\n",
    "int main() {\n",
    "    const float ref = 0.5f;\n",
    "    \n",
    "    // pointer for array of floats\n",
    "    float *d_out = 0;\n",
    "    \n",
    "    // allocate device memory for output array\n",
    "    cudaMalloc(&d_out, N*sizeof(float));\n",
    "    \n",
    "    // launch kernel (1d)\n",
    "    distanceKernel<<<N/TPB, TPB>>>(d_out, ref, N);\n",
    "    \n",
    "    cudaFree(d_out);\n",
    "    return 0;\n",
    "}\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b0bed-ecfd-4446-8b70-807daf0851ea",
   "metadata": {},
   "source": [
    "# GPGPU Progamming Considerations\n",
    "\n",
    "- GPU code execution is asyncronous by nature. You can never when a particular index will be calculated so there can be no data dependencies between indices.\n",
    "```C++\n",
    "// so this is not valid\n",
    "d[i] = (d[i-1] + d[i-2]) / 2\n",
    "```\n",
    "- GPUs have their own memory, often referred to as VRAM. When writing code in C, C++, or FORTRAN, you must take care to manage both CPU and GPU memory to get the maximum performance regardless of what paradigm you are using. You can generally follow this process:\n",
    "    - Allocate memory on device (`cudaMalloc`) and host (`malloc`)\n",
    "    - *Prepare data on host*\n",
    "    - Transfer data from host to device (`cudaMemcpy`, direction: `cudaMemcpyHostToDevice`)\n",
    "    - *Process data on device*\n",
    "    - Transfer results back to host (`cudaMemcpy`, direction: `cudaMemcpyDeviceToHost`)\n",
    "    - Free memory on device (`cudaFree`) and host (`free`) \n",
    "    - With OpenACC and OpenCL, you can are able to avoid many of this as they manage the device memory for you; however, in many cases you can end up with slower code unless you add `data` directives as they manage it conservatively. \n",
    "- For data that is required for many calculations, it is better to persist it on the GPU if possible. Transfer of data from host to device is very slow and can easily overshadow any speedup benefits you get if not performed intelligently.\n",
    "\n",
    "| ![gpu memory bandwidth](./figures/memory_bandwidth.jpg) |\n",
    "|:--:|\n",
    "| Figure Credit: https://github.com/maxim-belkin/how_to_gpu_in_python |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123202b3-eb02-4b82-961f-f6b35f9563b6",
   "metadata": {},
   "source": [
    "# OpenACC\n",
    "\n",
    "- OpenACC is an attractive alternative to CUDA as it enables directives based code acceleration similar to OpenMP\n",
    "- The NVIDIA HPC SDK, formerly the PGI compiler suite, allows users to compile code for GPUs that is **very** similar to CPU specific code. \n",
    "    - On Henry2, I have had success using the `PrgEnv-pgi/19.4` module to compile code for GPUs\n",
    "- Rather than separating the body of a loop into a separate function that can only run on the device, converting a loop to a grid for a GPU can be as simple as adding the `#pragma acc kernels` directive before the loop and the compiler will do the rest. \n",
    "- Additionally, by adding the `-ta=multicore` compiler flag you can compile the same code for a CPU and it will run similarly to as if you had used OpenMP.\n",
    "    - This means that you can write your code once, but compile a version that can run on a GPU and a version that can run without a GPU, both of which will be parallelized.\n",
    "- Further, rather than being tied to NVIDIA Hardware (as you are with CUDA), you can use the `-ta=radeon` flag to compile code that can be executed on AMD GPUs\n",
    "    - NOTE: I have not tested this.\n",
    "- The [OpenACC Programming and Best Practices Guide](https://www.openacc.org/sites/default/files/inline-files/openacc-guide.pdf) is a great place to start learning about OpenACC.\n",
    "- The [OpenACC Specifications](https://www.openacc.org/specification) are where you want to look for detailed information about all the functionality it provides. \n",
    "- You can also find two OpenACC examples in the `examples/c++/openacc` folder, both of which include submission files that can be used to run the programs on Henry2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072825b-441d-4048-af40-51a0fb9ef16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
